# Autonomy overview {#autonomy_overview}

Assigned: Liam

In this chapter we will introduce some basic concepts ubiquitous in autonomous vehicle navigation. 


## Autonomous Vehicles in the News {#autonomous-vehicles-news}




## Levels of Autonomy {#autonomy-levels}




## Basic Building Blocks of Autonomy {#basic-blocks}

The minimal basic backbone processing pipeline for autonomous vehicle navigation is shown in [](##fig:autonomy_block_diagram).

<div figure-id="fig:autonomy_block_diagram" figure-caption="The basic building blocks of any autonomous vehicle">
  <img src="autonomy_overview_block.jpg" style='width: 30em'/>
</div>

For an autonomous vehicle to function, it must achieve some level of performance for all of these components. The level of performance required depends on the *task* and the *required performance* .In the remainder of this section, we will discuss some of the most basic options. In [the next section](#advanced-blocks) we will briefly introduce some of the more advanced options that are used in state-of-the-art autonomous vehicles.


### Sensors {#sensors}

\begin{definition}[Sensor]\label{def:sensor}
A *sensor* is a device that or mechanism that is capable of generating a measurement of some external phyiscal quantity
\end{definition}

In general, sensors have two major types. *Passive* sensors generate measurements without affecting the environment that they are measuring. Examples include inertial sensors, odometers, GPS receivers, and cameras. *Active* sensors emit some form of energy into the environment in order to make a measurement. Examples of this type of sensor include Light Detection And Ranging (LiDAR), Radio Detection And Ranging (RaDAR), and Sound Navigation and Ranging (SoNAR). All of these sensors emit energy (from different spectra) into the environment and then detect some property of the energy that is reflected from the environment (e.g., the time of flight or the phase shift of the signal)


### Raw Data Processing {#data-processing}

The raw data that is input from a sensor needs to be  processed in order to become useful and even understandandable to a human. 

First, **calibration** is usually required to convert convert units, for example from a voltage to a physical quantity. As a simple example consider a thermometer, which measures temperature via an expanding liquid (usually mercury). The calibration is the known mapping from amount of expansion of liquid to temperature. In this case it is a linear mapping and is used to put the markings on the thermometer that make it useful as a sensor.

We will distiguish between two fundamentally types of calibrations.

\begin{definition}[Intrinsic Calibration]{def:intrinsic-calibration}
An *intrinsic calibration* is required to determine sensor-specific paramaters that are internal to a specific sensor.
\end{definition}

\begin{definition}[Extrinsic Calibration]{def:extrinsic-calibration}
An *extrinsic calibration* is required to determine the external configuration of the sensor with respect to some reference frame.
\end{definition}

<div class="check" markdown="1">
For more information about reference frames check out [](#reference_frames)
</div>

Calibration is very important consideration in robotics. In the field, the most advanced algorithms will fail if sensors are not properly calibrated.


Once we have properly calibrated data in some meaningful units, we often do some preprocessing to reduce the overall size of the data. This is true particularly for sensors that generate a lot of data, like cameras. Rather than deal with every pixel value generated by the camera, we will process an image to generate feature-points of interest. In ``classical" computer vision many different feature descriptors have been proposed (Harris, BRIEF, BRISK, SURF, SIFT, etc), and more recently Convolutional Neural Networks (CNNs) are being used to learn these features.

The important property of these features is that they should be as easily to associate as possible across frames. In order to achieve this, the feature descriptors should be invariant to nuissance parameters.

TODO: add a picture with basic feature detections


### State Estimation {#state-estimation}

Now that we have used our sensors to generate a set of meaningful measurements, we need to combine these measurements together to produce an estimate of the underlying hidden state of the robot and possibly to environment.


### Planning and Control {#planning-control}

State->control


### Actuation {#actuation}

Control applied to vehicle


### Infrasctructure and Prior Information {#infrastructure}





## Advanced Building Blocks of Autonomy {#advanced-blocks}


